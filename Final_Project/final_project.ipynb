{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "import ssl\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "import wandb.integration\n",
    "\n",
    "from cleantext.clean import clean\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nltk():\n",
    "    \"\"\"Initialize NLTK resources if they don't exist.\"\"\"\n",
    "    try:\n",
    "        # Check if resources are already downloaded\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "        nltk.data.find('corpora/omw-1.4')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        # Only download if resources are missing\n",
    "        try:\n",
    "            _create_unverified_https_context = ssl._create_unverified_context\n",
    "            ssl._create_default_https_context = _create_unverified_https_context\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('omw-1.4', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize NLTK resources\n",
    "initialize_nltk()\n",
    "\n",
    "uniqueWordsBeforePreprocessed = Counter()\n",
    "uniqueWordsAfterCleaned = Counter()\n",
    "uniqueWordsAfterRemStopWords = Counter()\n",
    "uniqueWordsAfterStemming = Counter()\n",
    "\n",
    "allWords = 0\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "date_patterns = re.compile(\n",
    "    r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\.?\\s+\\d+,?\\s+\\d+\\b|'\n",
    "    r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d+,?\\s+\\d+\\b|'\n",
    "    r'(\\d+)-(\\d+)-(\\d+) ?(\\d*):?(\\d*):?(\\d*)(\\.\\d+)?'\n",
    ")\n",
    "\n",
    "def clean_column(text):\n",
    "    '''\n",
    "    Here to clean the text data, we are using the cleantext library.\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    global uniqueWordsBeforePreprocessed\n",
    "    uniqueWordsBeforePreprocessed.update(text.split())\n",
    "    text = text.lower()\n",
    "    text = re.sub(date_patterns, ' <DATE> ', text)\n",
    "\n",
    "    text = clean(text,\n",
    "            fix_unicode=True,\n",
    "            to_ascii=True, \n",
    "            no_punct=True,\n",
    "            no_urls=True,                  \n",
    "            no_emails=True,                \n",
    "            no_numbers=True,  \n",
    "            replace_with_punct= \"\",             \n",
    "            no_line_breaks=True,\n",
    "            replace_with_url=\" <URL> \",\n",
    "            replace_with_email=\" <EMAIL> \",\n",
    "            replace_with_number=\" <NUMBER> \",\n",
    "            lower=True\n",
    "            )\n",
    "    \n",
    "    global uniqueWordsAfterCleaned\n",
    "    uniqueWordsAfterCleaned.update(text.split())\n",
    "    return text\n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    This function will clear stop words, assuming the text has been cleaned.\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    global uniqueWordsAfterRemStopWords\n",
    "    uniqueWordsAfterRemStopWords.update(filtered_tokens)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def stem_text(text):\n",
    "    '''\n",
    "    This function will perform stemming, assuming the text has been cleaned.\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    tokens = text.split(' ')\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    global uniqueWordsAfterStemming\n",
    "    uniqueWordsAfterStemming.update(stemmed_tokens)\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process a single text entry (cleaning, stopwords removal, stemming).\n",
    "    \"\"\"\n",
    "    text = clean_column(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    return text\n",
    "\n",
    "def process_df(df):\n",
    "    \"\"\"Process the entire DataFrame.\"\"\"\n",
    "    df['content'] = df['content'].apply(process_text)\n",
    "    return df\n",
    "\n",
    "df_chunks = pd.read_csv(\"995,000_rows.csv\", usecols=['type','content'], chunksize=10000)\n",
    "\n",
    "with open(\"processed.csv\", 'w', encoding='utf-8') as f:\n",
    "    for chunk in df_chunks:\n",
    "        processed_chunk = process_df(chunk)\n",
    "        processed_chunk.to_csv(f, index=False, header=f.tell()==0)\n",
    "print(f\"Cleaned data saved to processed.csv\")\n",
    "print(\"Unique words before preprocessing: \", len(uniqueWordsBeforePreprocessed))\n",
    "print(\"Unique words after cleaning: \", len(uniqueWordsAfterCleaned))\n",
    "print(\"Unique words after removing stop words: \", len(uniqueWordsAfterRemStopWords))\n",
    "print(\"Unique words after stemming: \", len(uniqueWordsAfterStemming))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"processed.csv\"\n",
    "\n",
    "try:\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(input_csv, chunksize=10000):\n",
    "        chunks.append(chunk)\n",
    "    df = pd.concat(chunks)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{input_csv}' not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "train_df.to_csv('995_train.csv', index = False)\n",
    "val_df.to_csv('995_validation.csv', index=False)\n",
    "test_df.to_csv('995_test.csv', index = False)\n",
    "\n",
    "print('processed.csv has succesfully been split into train.csv, validation.csv, test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "def label_entry(type, labels):\n",
    "    \"\"\"\n",
    "    Label one document as fake or reliable, defaults to None (which can be removed with .dropna()).\n",
    "    \"\"\"\n",
    "    if type in labels[0]:\n",
    "        return 0\n",
    "    elif type in labels[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        print(f\"MISSED TYPE:{type}\")\n",
    "        return None\n",
    "\n",
    "def logistic_regression(train_x:pd.Series, train_y:pd.Series, labels):\n",
    "    \"\"\"\n",
    "    Initialize an SKLearn Logistic regression model.\n",
    "    \"\"\"\n",
    "    vector_col = vectorizer.fit_transform(train_x).astype(np.float32)\n",
    "    label_col = train_y\n",
    "    print(\"Done vectorizing data.\")\n",
    "    model = LogisticRegression(class_weight=\"balanced\", random_state=16, solver=\"saga\", max_iter=1000, n_jobs=-1)\n",
    "    print(\"Done iterating data.\")\n",
    "    model.fit(vector_col, label_col)\n",
    "    print(\"Done training model.\")\n",
    "    return model\n",
    "\n",
    "def test_model_lr(model:LogisticRegression, val_x:pd.Series, val_y:pd.Series, test_X=None, test_Y=None, wandb_init = False):\n",
    "    label_col = val_y\n",
    "    vector_col = vectorizer.transform(val_x).astype(np.float32)\n",
    "\n",
    "    # Evaluate scores:\n",
    "    val_predict = model.predict(vector_col)\n",
    "    val_score = f1_score(label_col, val_predict)\n",
    "    class_report_lr = classification_report(label_col, val_predict)\n",
    "    print(\"CLASS REPORT VALIDATION DATA:\")\n",
    "    print(class_report_lr)\n",
    "\n",
    "    if wandb_init:\n",
    "        wandb.init(project=\"gds-project-test\")\n",
    "        wandb.log({ \"val_accuracy\": val_score,})\n",
    "        wandb.finish()\n",
    "\n",
    "    if test_X is not None and test_Y is not None:\n",
    "        vector_col_test = vectorizer.transform(test_X).astype(np.float32)\n",
    "        label_col_test = test_Y\n",
    "        test_predict = model.predict(vector_col_test)\n",
    "        test_score = f1_score(label_col_test, test_predict)\n",
    "        return val_score, test_score\n",
    "    else:\n",
    "        return val_score\n",
    "\n",
    "'''\n",
    "Preparing training, validation, and test cases for the model\n",
    "'''\n",
    "labels = ([\"fake\", \"satire\", \"bias\", \"conspiracy\", \"junksci\", \"hate\", \"state\"],\n",
    "    [\"clickbait\", \"political\", \"reliable\"])\n",
    "unwanted_labels = [\"unreliable\", \"rumor\", \"unknown\", \"2018-02-10 13:43:39.521661\"]\n",
    "\n",
    "#split corpus data\n",
    "train = pd.read_csv(\"995_train.csv\").dropna()\n",
    "train.drop(train.index[(train[\"type\"].isin(unwanted_labels))],axis=0,inplace=True)\n",
    "train_X = train['content']\n",
    "train_Y = train['type'].apply(label_entry, args=(labels,)).to_numpy().astype(np.float32)\n",
    "\n",
    "validation = pd.read_csv(\"995_validation.csv\").dropna()\n",
    "validation.drop(validation.index[(validation[\"type\"].isin(unwanted_labels))],axis=0,inplace=True)\n",
    "validation_X = validation['content']\n",
    "validation_Y = validation['type'].apply(label_entry, args=(labels,)).to_numpy().astype(np.float32)\n",
    "\n",
    "test = pd.read_csv(\"995_test.csv\").dropna()\n",
    "test.drop(test.index[(test[\"type\"].isin(unwanted_labels))],axis=0,inplace=True)\n",
    "test_X = test['content']\n",
    "test_Y = test['type'].apply(label_entry, args=(labels,)).to_numpy().astype(np.float32)\n",
    "\n",
    "bbc_data = pd.read_csv(\"bbc_processed.csv\").dropna()\n",
    "bbc_data_X = bbc_data['content']\n",
    "bbc_data_Y = bbc_data['type'].apply(label_entry, args=(labels,)).to_numpy().astype(np.float32)\n",
    "\n",
    "X_val_bbc = pd.concat([validation_X, bbc_data_X])\n",
    "Y_val_bbc = np.concat([validation_Y, bbc_data_Y])\n",
    "print(\"Done getting data ready.\")\n",
    "\n",
    "model = logistic_regression(train_X, train_Y, labels)\n",
    "print(\"Done training model.\")\n",
    "val_score = test_model_lr(model, validation_X, validation_Y)\n",
    "bbc_val_score = test_model_lr(model, X_val_bbc, Y_val_bbc)\n",
    "fake_news_test_lr = test_model_lr(model, test_X, test_Y)\n",
    "\n",
    "print(f\"f1 validation score without bbc data: {val_score}\")\n",
    "print(f\"f1 validation score with bbc data: {bbc_val_score}\")\n",
    "print(f\"f1 test score: {fake_news_test_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "def neural_network(train_x:pd.Series, train_y:pd.Series, labels):\n",
    "    \"\"\"\n",
    "    Initialize an SKLearn classifier neural network.\n",
    "    \"\"\"\n",
    "    classifier = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(50,), activation='relu', random_state=1, max_iter=500, n_features_in_=6)\n",
    "\n",
    "    vector_col = vectorizer.fit_transform(train_x)\n",
    "    label_col = train_y.to_numpy()\n",
    "    print(\"Done vectorizing data.\")\n",
    "    classifier.fit(vector_col, label_col)\n",
    "    print(\"Done training model.\")\n",
    "    return classifier\n",
    "\n",
    "def test_model_nn(model:MLPClassifier, val_x:pd.Series, val_y:pd.Series, test_X=None, test_Y=None, wandb_init = False):\n",
    "    label_col = val_y.to_numpy()\n",
    "    vector_col = vectorizer.transform(val_x)\n",
    "\n",
    "    # Evaluate scores:\n",
    "    val_pred = model.predict(vector_col)\n",
    "    val_f1 = f1_score(label_col, val_pred, average='weighted')\n",
    "    class_report = classification_report(label_col, val_pred)\n",
    "    print(\"CLASS REPORT VALIDATION DATA:\")\n",
    "    print(class_report)\n",
    "\n",
    "    if wandb_init:\n",
    "        wandb.init(project=\"gds-project-test\")\n",
    "        wandb.log({\"val_f1\": val_f1})\n",
    "        wandb.finish()\n",
    "\n",
    "    if test_X is not None and test_Y is not None:\n",
    "        vector_col_test = vectorizer.transform(test_X)\n",
    "        label_col_test = test_Y.to_numpy()\n",
    "        test_pred = model.predict(vector_col_test)\n",
    "        test_f1 = f1_score(label_col_test, test_pred, average='weighted')\n",
    "        class_report_test = classification_report(label_col, val_pred)\n",
    "        print(\"CLASS REPORT TEST DATA:\")\n",
    "        print(class_report_test)\n",
    "        return test_f1\n",
    "    else:\n",
    "        return val_f1\n",
    "    \n",
    "def liar_file(model:MLPClassifier, liar_file_train, liar_file_val, liar_file_test, wandb_init = None):\n",
    "    train_y = liar_file_train[1]\n",
    "    columns = [3, 4, 5, 6]\n",
    "\n",
    "    df = liar_file_train[2]\n",
    "    for column in columns:\n",
    "        df = pd.concat([df, liar_file_train[column]], axis=1)\n",
    "\n",
    "    pt.process_df(df)\n",
    "    train_x = vectorizer.transform(df)\n",
    "    prediction = model.predict(train_x)\n",
    "    score = model.score(train_y, prediction)\n",
    "\n",
    "    return score\n",
    "\n",
    "\"\"\"\n",
    "Show test-case.\n",
    "\"\"\"\n",
    "labels = ([\"fake\", \"satire\", \"bias\", \"conspiracy\", \"junksci\", \"state\"],\n",
    "    [\"clickbait\", \"political\", \"reliable\"])\n",
    "unwanted_labels = [\"unreliable\", \"rumor\", \"unknown\", \"hate\", \"2018-02-10 13:43:39.521661\"]\n",
    "\n",
    "#split corpus data\n",
    "data = pd.read_csv(\"processed.csv\").dropna()\n",
    "data.drop(data.index[(data[\"type\"].isin(unwanted_labels))],axis=0,inplace=True)\n",
    "data_X = data['content']\n",
    "data_Y = data['type'].apply(label_entry, args=(labels,))\n",
    "print(\"Done getting data ready.\")\n",
    "\n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=16)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5, random_state=16)\n",
    "print(\"Done splitting data.\")\n",
    "\n",
    "model = neural_network(X_train, Y_train, labels)\n",
    "print(\"Done training model.\")\n",
    "\n",
    "fake_news_test_nn = test_model_nn(model, X_val, Y_val)\n",
    "print(f\"VALIDATION DATA F1: {test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model evaluated with LIAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_liar_label(label):\n",
    "    if label in [\"pants-fire\", \"false\", \"barely-true\", \"half-true\"]:\n",
    "        return 0\n",
    "    elif label in [\"mostly-true\", \"true\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "liar_train_data_lr = pd.read_csv(\"liar_dataset/train.tsv\", sep='\\t', header=None)\n",
    "liar_train_X = liar_train_data_lr[[2, 3, 4, 5, 6, 7]].fillna('').agg('\\t'.join, axis=1)\n",
    "liar_train_Y = liar_train_data_lr[1].apply(map_liar_label)\n",
    "liar_valid_lr_test = liar_train_Y.notnull()\n",
    "liar_train_X = liar_train_X[liar_valid_lr_test]\n",
    "liar_train_X = liar_train_X.apply(process_text)\n",
    "liar_train_Y = liar_train_Y[liar_valid_lr_test]\n",
    "\n",
    "liar_val_data_lr = pd.read_csv(\"liar_dataset/valid.tsv\", sep='\\t', header=None)\n",
    "liar_val_X = liar_val_data_lr[[2, 3, 4, 5, 6, 7]].fillna('').agg('\\t'.join, axis=1)\n",
    "liar_val_Y = liar_val_data_lr[1].apply(map_liar_label)\n",
    "liar_valid_lr_val = liar_val_Y.notnull()\n",
    "liar_val_X = liar_val_X[liar_valid_lr_val]\n",
    "liar_val_X = liar_val_X.apply(process_text)\n",
    "liar_val_Y = liar_val_Y[liar_valid_lr_val]\n",
    "\n",
    "liar_test_data_lr = pd.read_csv(\"liar_dataset/test.tsv\", sep='\\t', header=None)\n",
    "liar_test_X = liar_test_data_lr[[2, 3, 4, 5, 6, 7]].fillna('').agg('\\t'.join, axis=1)\n",
    "liar_test_Y = liar_test_data_lr[1].apply(map_liar_label)\n",
    "liar_valid_lr_test = liar_test_Y.notnull()\n",
    "liar_test_X = liar_test_X[liar_valid_lr_val]\n",
    "liar_test_X = liar_test_X.apply(process_text)\n",
    "liar_test_Y = liar_test_Y[liar_valid_lr_val]\n",
    "\n",
    "print(\"Evaluating FakeNewsCorpus-trained model on LIAR dataset\")\n",
    "liar_f1_lr_train = test_model_lr(model, liar_train_X, liar_train_Y)\n",
    "liar_f1_lr_val = test_model_lr(model, liar_val_X, liar_val_Y)\n",
    "liar_f1_lr_test = test_model_lr(model, liar_test_X, liar_test_Y)\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) train set: {liar_f1_lr_train}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) validation set: {liar_f1_lr_val}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) test set: {liar_f1_lr_test}\")\n",
    "\n",
    "# TASK 3: Comparison\n",
    "print(\"Comparison of Results:\")\n",
    "print(f\"FakeNewsCorpus Validation F1: {fake_news_test_lr}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) train set: {liar_f1_lr_train}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) validation set: {liar_f1_lr_val}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) test set: {liar_f1_lr_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network model evaluated with LIAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating FakeNewsCorpus-trained model on LIAR dataset\")\n",
    "liar_f1_nn_train = test_model_nn(model, liar_train_X, liar_train_Y)\n",
    "liar_f1_nn_val = test_model_nn(model, liar_val_X, liar_val_Y)\n",
    "liar_f1_nn_test = test_model_nn(model, liar_test_X, liar_test_Y)\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) train set: {liar_f1_nn_train}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) validation set: {liar_f1_nn_val}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) test set: {liar_f1_nn_test}\")\n",
    "\n",
    "# TASK 3: Comparison\n",
    "print(\"Comparison of Results:\")\n",
    "print(f\"FakeNewsCorpus Validation F1: {fake_news_test_nn}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) train set: {liar_f1_nn_train}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) validation set: {liar_f1_nn_val}\")\n",
    "print(f\"LIAR Dataset F1 (Cross-domain) test set: {liar_f1_nn_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
